
Requirements: Python 3.6.6+, scipy

Code structure:

dicts/: dictionary files that each contains every possible token in a model, one line per token.
core/unigram.py: contains the function for training (MLE,MAP,predictive distribution) 
				 and function that calculates log of evidence function.
core/metrics.py: contains the function that calculates perplexity.
utils/text.py: contains the function that generates token sequence from text file (as a generator), 
			   and the function that convert dictionary file to list.
alphaselect.py: executable; see below.
unigrampp.py: executable; see below.

------------------------------------------------------------------------

unigrampp.py: training models, or test on test set using trained models, calculate and output perplexity.

usage: python3 unigrampp.py DATAFILE [--datasize DATASIZE]
                    [--model MODEL] [--training] [--method METHOD]
                    [--dict DICT] [--savepath SAVEPATH] [--alphap ALPHAP]
                    [--pp]

Common arguments:
  DATAFILE  file path of training/testing data.
  --datasize DATASIZE  size(in count of tokens) of the data set to be used; default is to use the entire file.
					   
Training mode arguments:					   
  --training           run in training mode; default is False
  --method METHOD      training method, could be 'MLE','MAP or 'PD'
  --dict DICT          path to the word dictionary, containing one token each line
  --savepath SAVEPATH  specify the file path to save the model after training; default is "[TRAINING METHOD]_[DATASIZE].model"
  --alphap ALPHAP      specify value of alpha prime; default is 2
  --pp                 calculate and display perplexity of the model on
                       training set after training; default is False
					   
Example: python3 unigrampp.py training_data.txt --training --datasize 160000 --method MLE --dict dicts/dict.txt --savepath MLE_160000.model --pp

This uses the MLE method to train on training_data.txt; using the first 160000 tokens from the file; 
use dicts/dict.txt as word dictionary; save the trained model to MLE_160000.model; 
and calculate and display perplexity after training.				

					
Testing mode argument:
  --model MODEL        specify file path of an existing model to be used for
                       testing.

Example: python3 unigrampp.py test_data.txt --model models/MLE_160000.model  
This uses the model file MLE_160000.model to calculate and output the perplexity for test_data.txt.

------------------------------------------------------------------------

alphaselect.py: try different alpha from 1-10, train model using predictive distribution, 
				and calculate perplexity and log of evidence function of the model on specified test set.

usage:python3 alphaselect.py [-h] [--dict DICT] [--training_set TRAINING_SET]
                      [--training_size TRAINING_SIZE] [--test_set TEST_SET]
                      [--test_size TEST_SIZE]

arguments:
  -h, --help            show this help message and exit
  --dict DICT           path of the word dictionary, containing one token each line
  --training_set TRAINING_SET
                        path of the training data file
  --training_size TRAINING_SIZE
                        size(count of tokens) of training file to use
  --test_set TEST_SET   path of the testing data file
  --test_size TEST_SIZE
                        size(count of tokens) of testing file to use
						
Example:
python3 alphaselect.py --dict dicts/dict.txt --training_set training_data.txt --training_size 5000 --test_set test_data.txt --test_size 640000
